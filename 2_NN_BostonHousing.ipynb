{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_NN_BostonHousing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USEI9z7a5LEX",
        "colab_type": "text"
      },
      "source": [
        "Load Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hzqWFDc5Nmf",
        "colab_type": "code",
        "outputId": "1544f333-5b2c-47ef-9e82-c3b0127e493f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.0-rc2'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_GPRl3k5WpB",
        "colab_type": "text"
      },
      "source": [
        "Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LABxpeXs5R3j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#The data set is available with TF library\n",
        "(train_x,train_y),(_,_) = tf.keras.datasets.boston_housing.load_data(test_split=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Zazv55z5rAq",
        "colab_type": "code",
        "outputId": "034bf3b2-a134-4def-e9b8-1e3206aef8c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(train_x)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9GyHoKY5vIS",
        "colab_type": "code",
        "outputId": "f4f8f94d-7554-401d-c0d8-be8dbce6a49c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Check how many training examples we have\n",
        "train_x.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(506, 13)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfzScye258dT",
        "colab_type": "code",
        "outputId": "7e8e7ad1-f41e-4089-ca70-d761e4d01e2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_x.dtype"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('float64')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCuGJiYt6By6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ML works with floating point numbers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h9bhG5N6J6D",
        "colab_type": "code",
        "outputId": "906832ac-8c73-4397-b556-f1aa1971f411",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Data type of features\n",
        "train_x.dtype"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('float64')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25ijdr8N6NRh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#In this case each floating point numbers uses 64 bits to store in memory\n",
        "#By default Numpy uses float64\n",
        "#To reduce the memory requirement. TF and DL Libraries use float32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxfQzM_I6dZa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Convert x and y to 32 bits\n",
        "train_x = train_x.astype('float32')\n",
        "train_y = train_y.astype('float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeV50s5d6-Oj",
        "colab_type": "code",
        "outputId": "eede990c-3c4b-4957-da4d-899b0a7a5e3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "train_x[0:5]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.23247e+00, 0.00000e+00, 8.14000e+00, 0.00000e+00, 5.38000e-01,\n",
              "        6.14200e+00, 9.17000e+01, 3.97690e+00, 4.00000e+00, 3.07000e+02,\n",
              "        2.10000e+01, 3.96900e+02, 1.87200e+01],\n",
              "       [2.17700e-02, 8.25000e+01, 2.03000e+00, 0.00000e+00, 4.15000e-01,\n",
              "        7.61000e+00, 1.57000e+01, 6.27000e+00, 2.00000e+00, 3.48000e+02,\n",
              "        1.47000e+01, 3.95380e+02, 3.11000e+00],\n",
              "       [4.89822e+00, 0.00000e+00, 1.81000e+01, 0.00000e+00, 6.31000e-01,\n",
              "        4.97000e+00, 1.00000e+02, 1.33250e+00, 2.40000e+01, 6.66000e+02,\n",
              "        2.02000e+01, 3.75520e+02, 3.26000e+00],\n",
              "       [3.96100e-02, 0.00000e+00, 5.19000e+00, 0.00000e+00, 5.15000e-01,\n",
              "        6.03700e+00, 3.45000e+01, 5.98530e+00, 5.00000e+00, 2.24000e+02,\n",
              "        2.02000e+01, 3.96900e+02, 8.01000e+00],\n",
              "       [3.69311e+00, 0.00000e+00, 1.81000e+01, 0.00000e+00, 7.13000e-01,\n",
              "        6.37600e+00, 8.84000e+01, 2.56710e+00, 2.40000e+01, 6.66000e+02,\n",
              "        2.02000e+01, 3.91430e+02, 1.46500e+01]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJpSwJrj60Sk",
        "colab_type": "text"
      },
      "source": [
        "Build Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukHFedTg7Fst",
        "colab_type": "text"
      },
      "source": [
        "Define weights and bias\n",
        "\n",
        "We need weights and bias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eij7BXVQ62PC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create weights and bias and initialize with Zero\n",
        "w = tf.zeros(shape =(13,1))\n",
        "b= tf.zeros(shape = (1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayPxzCz5aDUA",
        "colab_type": "text"
      },
      "source": [
        "Define a function to calculate prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ia0kh77waCqg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Calculate Y\n",
        "#1. Multiply x and w (Matrix mulitplication)\n",
        "#2. Shape of X [506,13]. Shape of Y [13,1]. Shape of the  product will be [560,1]\n",
        "#3. Add the bias"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdKcrhqGaxf2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prediction(x,w,b):\n",
        "    xw_matrixMultiplication = tf.matmul(x,w)\n",
        "    y = tf.add(xw_matrixMultiplication, b)\n",
        "    return y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoCzkwmgbNhv",
        "colab_type": "text"
      },
      "source": [
        "Calculate Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "877ss56CbO2J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Function to calculate Loss (Mean Squared Error)\n",
        "\n",
        "def loss(y_actual, y_predicted):\n",
        "  diff= y_actual - y_predicted\n",
        "  sqr = tf.square(diff)\n",
        "  avg = tf.reduce_mean(sqr)\n",
        "\n",
        "  return avg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vA6Bqta_cDzU",
        "colab_type": "text"
      },
      "source": [
        "Calculate Gradient Descent in TF\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UptjuusXcI7U",
        "colab_type": "text"
      },
      "source": [
        "Function to train the model\n",
        "1. Record all the mathematical steps to calculate Loss. The steps will be recorded using GradientTape\n",
        "\n",
        "2. Calculate Gradients of Loss w.r.t weights and bias\n",
        "\n",
        "3. Update weights and bias based on gradients and learning rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbDGp9nZcLih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(x, y_actual, w, b, learning_rate=0.01):\n",
        "    \n",
        "    #Record mathematical operations on 'tape' to calculate loss\n",
        "    with tf.GradientTape() as t:\n",
        "        \n",
        "        t.watch([w,b])\n",
        "        \n",
        "        current_prediction = prediction(x, w, b)\n",
        "        current_loss = loss(y_actual, current_prediction)\n",
        "    \n",
        "    #Calculate Gradients for Loss with respect to Weights and Bias\n",
        "    dw, db = t.gradient(current_loss,[w, b])\n",
        "    \n",
        "    #Update Weights and Bias\n",
        "    w = w - learning_rate*dw\n",
        "    b = b - learning_rate*db\n",
        "    \n",
        "    return w, b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1ao18h7yll6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ebab5ace-0ffc-4022-bbc6-3464193838ac"
      },
      "source": [
        "#Train for 100 Steps\n",
        "for i in range(100):\n",
        "    \n",
        "    w, b = train(train_x, train_y, w, b, learning_rate=0.01)\n",
        "    print('Current Loss on iteration', i, \n",
        "          loss(train_y, prediction(train_x, w, b)).numpy())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current Loss on iteration 0 19006894000.0\n",
            "Current Loss on iteration 1 7.4460295e+17\n",
            "Current Loss on iteration 2 2.917326e+25\n",
            "Current Loss on iteration 3 1.1429984e+33\n",
            "Current Loss on iteration 4 inf\n",
            "Current Loss on iteration 5 inf\n",
            "Current Loss on iteration 6 inf\n",
            "Current Loss on iteration 7 inf\n",
            "Current Loss on iteration 8 inf\n",
            "Current Loss on iteration 9 inf\n",
            "Current Loss on iteration 10 nan\n",
            "Current Loss on iteration 11 nan\n",
            "Current Loss on iteration 12 nan\n",
            "Current Loss on iteration 13 nan\n",
            "Current Loss on iteration 14 nan\n",
            "Current Loss on iteration 15 nan\n",
            "Current Loss on iteration 16 nan\n",
            "Current Loss on iteration 17 nan\n",
            "Current Loss on iteration 18 nan\n",
            "Current Loss on iteration 19 nan\n",
            "Current Loss on iteration 20 nan\n",
            "Current Loss on iteration 21 nan\n",
            "Current Loss on iteration 22 nan\n",
            "Current Loss on iteration 23 nan\n",
            "Current Loss on iteration 24 nan\n",
            "Current Loss on iteration 25 nan\n",
            "Current Loss on iteration 26 nan\n",
            "Current Loss on iteration 27 nan\n",
            "Current Loss on iteration 28 nan\n",
            "Current Loss on iteration 29 nan\n",
            "Current Loss on iteration 30 nan\n",
            "Current Loss on iteration 31 nan\n",
            "Current Loss on iteration 32 nan\n",
            "Current Loss on iteration 33 nan\n",
            "Current Loss on iteration 34 nan\n",
            "Current Loss on iteration 35 nan\n",
            "Current Loss on iteration 36 nan\n",
            "Current Loss on iteration 37 nan\n",
            "Current Loss on iteration 38 nan\n",
            "Current Loss on iteration 39 nan\n",
            "Current Loss on iteration 40 nan\n",
            "Current Loss on iteration 41 nan\n",
            "Current Loss on iteration 42 nan\n",
            "Current Loss on iteration 43 nan\n",
            "Current Loss on iteration 44 nan\n",
            "Current Loss on iteration 45 nan\n",
            "Current Loss on iteration 46 nan\n",
            "Current Loss on iteration 47 nan\n",
            "Current Loss on iteration 48 nan\n",
            "Current Loss on iteration 49 nan\n",
            "Current Loss on iteration 50 nan\n",
            "Current Loss on iteration 51 nan\n",
            "Current Loss on iteration 52 nan\n",
            "Current Loss on iteration 53 nan\n",
            "Current Loss on iteration 54 nan\n",
            "Current Loss on iteration 55 nan\n",
            "Current Loss on iteration 56 nan\n",
            "Current Loss on iteration 57 nan\n",
            "Current Loss on iteration 58 nan\n",
            "Current Loss on iteration 59 nan\n",
            "Current Loss on iteration 60 nan\n",
            "Current Loss on iteration 61 nan\n",
            "Current Loss on iteration 62 nan\n",
            "Current Loss on iteration 63 nan\n",
            "Current Loss on iteration 64 nan\n",
            "Current Loss on iteration 65 nan\n",
            "Current Loss on iteration 66 nan\n",
            "Current Loss on iteration 67 nan\n",
            "Current Loss on iteration 68 nan\n",
            "Current Loss on iteration 69 nan\n",
            "Current Loss on iteration 70 nan\n",
            "Current Loss on iteration 71 nan\n",
            "Current Loss on iteration 72 nan\n",
            "Current Loss on iteration 73 nan\n",
            "Current Loss on iteration 74 nan\n",
            "Current Loss on iteration 75 nan\n",
            "Current Loss on iteration 76 nan\n",
            "Current Loss on iteration 77 nan\n",
            "Current Loss on iteration 78 nan\n",
            "Current Loss on iteration 79 nan\n",
            "Current Loss on iteration 80 nan\n",
            "Current Loss on iteration 81 nan\n",
            "Current Loss on iteration 82 nan\n",
            "Current Loss on iteration 83 nan\n",
            "Current Loss on iteration 84 nan\n",
            "Current Loss on iteration 85 nan\n",
            "Current Loss on iteration 86 nan\n",
            "Current Loss on iteration 87 nan\n",
            "Current Loss on iteration 88 nan\n",
            "Current Loss on iteration 89 nan\n",
            "Current Loss on iteration 90 nan\n",
            "Current Loss on iteration 91 nan\n",
            "Current Loss on iteration 92 nan\n",
            "Current Loss on iteration 93 nan\n",
            "Current Loss on iteration 94 nan\n",
            "Current Loss on iteration 95 nan\n",
            "Current Loss on iteration 96 nan\n",
            "Current Loss on iteration 97 nan\n",
            "Current Loss on iteration 98 nan\n",
            "Current Loss on iteration 99 nan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mw3ahBPyyqHx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "707e223f-d2f0-40c2-ce77-08e8ce8ec3ce"
      },
      "source": [
        "#Check Weights and Bias\n",
        "print('Weights:\\n', w.numpy())\n",
        "print('Bias:\\n',b.numpy())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Weights:\n",
            " [[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Bias:\n",
            " [nan]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFgvqYSz0Hxj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "7237041c-5fa1-4500-803e-a0daa6974f96"
      },
      "source": [
        "train_x[0]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  1.23247,   0.     ,   8.14   ,   0.     ,   0.538  ,   6.142  ,\n",
              "        91.7    ,   3.9769 ,   4.     , 307.     ,  21.     , 396.9    ,\n",
              "        18.72   ], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErFut8Q-0LBS",
        "colab_type": "text"
      },
      "source": [
        "The numbers are of different ranges. They are not normalized or scaled. Hence we got the result as nan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RncpZKmb1PCn",
        "colab_type": "text"
      },
      "source": [
        "Why Normalization is important.\n",
        "\n",
        "Consider two actual values of y as 300 and 0.1 and the predicted values be 290 and 0.4. \n",
        "\n",
        "The loss will be 100 amd 0.09 respectively.  0.3% and 300% respectively\n",
        "\n",
        "In this case during Gradient Descent, the importance will be given to higher value (100). So, the model will focus on larger or higher values and providing wrong results.\n",
        "\n",
        "Hence normalization is important."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpKzHmWF2Fkk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}