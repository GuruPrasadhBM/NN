{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_NN_BostonHousing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USEI9z7a5LEX",
        "colab_type": "text"
      },
      "source": [
        "Load Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hzqWFDc5Nmf",
        "colab_type": "code",
        "outputId": "3aaa81b2-1073-4e84-f4e4-40e1623db444",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.0-rc2'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_GPRl3k5WpB",
        "colab_type": "text"
      },
      "source": [
        "Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LABxpeXs5R3j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#The data set is available with TF library\n",
        "(train_x,train_y),(_,_) = tf.keras.datasets.boston_housing.load_data(test_split=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Zazv55z5rAq",
        "colab_type": "code",
        "outputId": "e80e9590-8640-45d6-d5f3-343fc53fd85f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(train_x)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9GyHoKY5vIS",
        "colab_type": "code",
        "outputId": "8516870e-d75f-4e78-9c97-b8dcbc8e2945",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Check how many training examples we have\n",
        "train_x.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(506, 13)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfzScye258dT",
        "colab_type": "code",
        "outputId": "ec851d12-b9a4-4c19-9834-6824f6c93f06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_x.dtype"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('float64')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCuGJiYt6By6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ML works with floating point numbers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h9bhG5N6J6D",
        "colab_type": "code",
        "outputId": "6eab2ec9-5a31-441c-d875-010436b7a853",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Data type of features\n",
        "train_x.dtype"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('float64')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25ijdr8N6NRh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#In this case each floating point numbers uses 64 bits to store in memory\n",
        "#By default Numpy uses float64\n",
        "#To reduce the memory requirement. TF and DL Libraries use float32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxfQzM_I6dZa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Convert x and y to 32 bits\n",
        "train_x = train_x.astype('float32')\n",
        "train_y = train_y.astype('float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeV50s5d6-Oj",
        "colab_type": "code",
        "outputId": "b549f541-1bf9-417d-ba4f-9998d5c79091",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "train_x[0:5]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.23247e+00, 0.00000e+00, 8.14000e+00, 0.00000e+00, 5.38000e-01,\n",
              "        6.14200e+00, 9.17000e+01, 3.97690e+00, 4.00000e+00, 3.07000e+02,\n",
              "        2.10000e+01, 3.96900e+02, 1.87200e+01],\n",
              "       [2.17700e-02, 8.25000e+01, 2.03000e+00, 0.00000e+00, 4.15000e-01,\n",
              "        7.61000e+00, 1.57000e+01, 6.27000e+00, 2.00000e+00, 3.48000e+02,\n",
              "        1.47000e+01, 3.95380e+02, 3.11000e+00],\n",
              "       [4.89822e+00, 0.00000e+00, 1.81000e+01, 0.00000e+00, 6.31000e-01,\n",
              "        4.97000e+00, 1.00000e+02, 1.33250e+00, 2.40000e+01, 6.66000e+02,\n",
              "        2.02000e+01, 3.75520e+02, 3.26000e+00],\n",
              "       [3.96100e-02, 0.00000e+00, 5.19000e+00, 0.00000e+00, 5.15000e-01,\n",
              "        6.03700e+00, 3.45000e+01, 5.98530e+00, 5.00000e+00, 2.24000e+02,\n",
              "        2.02000e+01, 3.96900e+02, 8.01000e+00],\n",
              "       [3.69311e+00, 0.00000e+00, 1.81000e+01, 0.00000e+00, 7.13000e-01,\n",
              "        6.37600e+00, 8.84000e+01, 2.56710e+00, 2.40000e+01, 6.66000e+02,\n",
              "        2.02000e+01, 3.91430e+02, 1.46500e+01]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNUSOBs83Gfc",
        "colab_type": "text"
      },
      "source": [
        "Normalize input features\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVz5_PUX3Y8m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "transformer = Normalizer()\n",
        "train_x = transformer.fit_transform(train_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haaFmJLO3s0j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d9e7ba19-f40f-41ad-a6d1-ddc2e40169fa"
      },
      "source": [
        "train_x[0]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.0024119 , 0.        , 0.01592969, 0.        , 0.00105285,\n",
              "       0.01201967, 0.17945358, 0.00778265, 0.00782786, 0.60078794,\n",
              "       0.04109624, 0.776719  , 0.03663436], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJpSwJrj60Sk",
        "colab_type": "text"
      },
      "source": [
        "Build Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukHFedTg7Fst",
        "colab_type": "text"
      },
      "source": [
        "Define weights and bias\n",
        "\n",
        "We need weights and bias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eij7BXVQ62PC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create weights and bias and initialize with Zero\n",
        "w = tf.zeros(shape =(13,1))\n",
        "b= tf.zeros(shape = (1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayPxzCz5aDUA",
        "colab_type": "text"
      },
      "source": [
        "Define a function to calculate prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ia0kh77waCqg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Calculate Y\n",
        "#1. Multiply x and w (Matrix mulitplication)\n",
        "#2. Shape of X [506,13]. Shape of Y [13,1]. Shape of the  product will be [560,1]\n",
        "#3. Add the bias"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdKcrhqGaxf2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prediction(x,w,b):\n",
        "    xw_matrixMultiplication = tf.matmul(x,w)\n",
        "    y = tf.add(xw_matrixMultiplication, b)\n",
        "    return y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoCzkwmgbNhv",
        "colab_type": "text"
      },
      "source": [
        "Calculate Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "877ss56CbO2J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Function to calculate Loss (Mean Squared Error)\n",
        "\n",
        "def loss(y_actual, y_predicted):\n",
        "  diff= y_actual - y_predicted\n",
        "  sqr = tf.square(diff)\n",
        "  avg = tf.reduce_mean(sqr)\n",
        "\n",
        "  return avg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vA6Bqta_cDzU",
        "colab_type": "text"
      },
      "source": [
        "Calculate Gradient Descent in TF\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UptjuusXcI7U",
        "colab_type": "text"
      },
      "source": [
        "Function to train the model\n",
        "1. Record all the mathematical steps to calculate Loss. The steps will be recorded using GradientTape\n",
        "\n",
        "2. Calculate Gradients of Loss w.r.t weights and bias\n",
        "\n",
        "3. Update weights and bias based on gradients and learning rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbDGp9nZcLih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(x, y_actual, w, b, learning_rate=0.01):\n",
        "    \n",
        "    #Record mathematical operations on 'tape' to calculate loss\n",
        "    with tf.GradientTape() as t:\n",
        "        \n",
        "        t.watch([w,b])\n",
        "        \n",
        "        current_prediction = prediction(x, w, b)\n",
        "        current_loss = loss(y_actual, current_prediction)\n",
        "    \n",
        "    #Calculate Gradients for Loss with respect to Weights and Bias\n",
        "    dw, db = t.gradient(current_loss,[w, b])\n",
        "    \n",
        "    #Update Weights and Bias\n",
        "    w = w - learning_rate*dw\n",
        "    b = b - learning_rate*db\n",
        "    \n",
        "    return w, b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1ao18h7yll6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8b70fd40-8693-4282-a1b7-58bee676bd63"
      },
      "source": [
        "#Train for 100 Steps\n",
        "for i in range(100):\n",
        "    \n",
        "    w, b = train(train_x, train_y, w, b, learning_rate=0.01)\n",
        "    print('Current Loss on iteration', i, \n",
        "          loss(train_y, prediction(train_x, w, b)).numpy())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current Loss on iteration 0 553.7515\n",
            "Current Loss on iteration 1 518.26166\n",
            "Current Loss on iteration 2 485.45786\n",
            "Current Loss on iteration 3 455.13657\n",
            "Current Loss on iteration 4 427.1098\n",
            "Current Loss on iteration 5 401.20413\n",
            "Current Loss on iteration 6 377.25894\n",
            "Current Loss on iteration 7 355.12595\n",
            "Current Loss on iteration 8 334.66785\n",
            "Current Loss on iteration 9 315.75797\n",
            "Current Loss on iteration 10 298.27924\n",
            "Current Loss on iteration 11 282.1232\n",
            "Current Loss on iteration 12 267.1898\n",
            "Current Loss on iteration 13 253.38655\n",
            "Current Loss on iteration 14 240.62785\n",
            "Current Loss on iteration 15 228.83476\n",
            "Current Loss on iteration 16 217.93404\n",
            "Current Loss on iteration 17 207.85832\n",
            "Current Loss on iteration 18 198.54506\n",
            "Current Loss on iteration 19 189.9366\n",
            "Current Loss on iteration 20 181.9796\n",
            "Current Loss on iteration 21 174.62473\n",
            "Current Loss on iteration 22 167.82646\n",
            "Current Loss on iteration 23 161.54263\n",
            "Current Loss on iteration 24 155.73433\n",
            "Current Loss on iteration 25 150.36557\n",
            "Current Loss on iteration 26 145.40309\n",
            "Current Loss on iteration 27 140.81613\n",
            "Current Loss on iteration 28 136.5763\n",
            "Current Loss on iteration 29 132.65727\n",
            "Current Loss on iteration 30 129.0348\n",
            "Current Loss on iteration 31 125.68646\n",
            "Current Loss on iteration 32 122.59149\n",
            "Current Loss on iteration 33 119.73073\n",
            "Current Loss on iteration 34 117.086426\n",
            "Current Loss on iteration 35 114.64222\n",
            "Current Loss on iteration 36 112.38296\n",
            "Current Loss on iteration 37 110.29462\n",
            "Current Loss on iteration 38 108.36431\n",
            "Current Loss on iteration 39 106.58005\n",
            "Current Loss on iteration 40 104.93081\n",
            "Current Loss on iteration 41 103.406334\n",
            "Current Loss on iteration 42 101.9972\n",
            "Current Loss on iteration 43 100.6947\n",
            "Current Loss on iteration 44 99.49073\n",
            "Current Loss on iteration 45 98.377846\n",
            "Current Loss on iteration 46 97.34915\n",
            "Current Loss on iteration 47 96.39827\n",
            "Current Loss on iteration 48 95.51935\n",
            "Current Loss on iteration 49 94.70689\n",
            "Current Loss on iteration 50 93.955894\n",
            "Current Loss on iteration 51 93.26171\n",
            "Current Loss on iteration 52 92.620026\n",
            "Current Loss on iteration 53 92.02687\n",
            "Current Loss on iteration 54 91.478584\n",
            "Current Loss on iteration 55 90.97176\n",
            "Current Loss on iteration 56 90.50326\n",
            "Current Loss on iteration 57 90.070175\n",
            "Current Loss on iteration 58 89.66985\n",
            "Current Loss on iteration 59 89.299805\n",
            "Current Loss on iteration 60 88.957726\n",
            "Current Loss on iteration 61 88.6415\n",
            "Current Loss on iteration 62 88.34917\n",
            "Current Loss on iteration 63 88.07895\n",
            "Current Loss on iteration 64 87.829155\n",
            "Current Loss on iteration 65 87.59823\n",
            "Current Loss on iteration 66 87.38475\n",
            "Current Loss on iteration 67 87.187386\n",
            "Current Loss on iteration 68 87.00495\n",
            "Current Loss on iteration 69 86.83627\n",
            "Current Loss on iteration 70 86.68036\n",
            "Current Loss on iteration 71 86.5362\n",
            "Current Loss on iteration 72 86.40293\n",
            "Current Loss on iteration 73 86.27969\n",
            "Current Loss on iteration 74 86.16578\n",
            "Current Loss on iteration 75 86.060455\n",
            "Current Loss on iteration 76 85.96308\n",
            "Current Loss on iteration 77 85.87303\n",
            "Current Loss on iteration 78 85.78977\n",
            "Current Loss on iteration 79 85.71281\n",
            "Current Loss on iteration 80 85.64161\n",
            "Current Loss on iteration 81 85.57579\n",
            "Current Loss on iteration 82 85.51492\n",
            "Current Loss on iteration 83 85.45864\n",
            "Current Loss on iteration 84 85.406586\n",
            "Current Loss on iteration 85 85.35844\n",
            "Current Loss on iteration 86 85.31391\n",
            "Current Loss on iteration 87 85.27272\n",
            "Current Loss on iteration 88 85.234634\n",
            "Current Loss on iteration 89 85.199394\n",
            "Current Loss on iteration 90 85.16679\n",
            "Current Loss on iteration 91 85.13663\n",
            "Current Loss on iteration 92 85.108734\n",
            "Current Loss on iteration 93 85.08291\n",
            "Current Loss on iteration 94 85.05902\n",
            "Current Loss on iteration 95 85.03692\n",
            "Current Loss on iteration 96 85.01646\n",
            "Current Loss on iteration 97 84.99751\n",
            "Current Loss on iteration 98 84.97998\n",
            "Current Loss on iteration 99 84.96375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mw3ahBPyyqHx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "943f5562-477e-410f-f671-953530b67f51"
      },
      "source": [
        "#Check Weights and Bias\n",
        "print('Weights:\\n', w.numpy())\n",
        "print('Bias:\\n',b.numpy())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Weights:\n",
            " [[6.27857521e-02]\n",
            " [2.58572161e-01]\n",
            " [2.17821732e-01]\n",
            " [1.46146421e-03]\n",
            " [1.13587305e-02]\n",
            " [1.31812572e-01]\n",
            " [1.38818812e+00]\n",
            " [8.23873580e-02]\n",
            " [1.76484972e-01]\n",
            " [7.99328279e+00]\n",
            " [3.82081807e-01]\n",
            " [7.41107655e+00]\n",
            " [2.53885090e-01]]\n",
            "Bias:\n",
            " [11.476417]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFgvqYSz0Hxj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "890de330-8140-4371-9b22-9ff1eb7263a8"
      },
      "source": [
        "train_x[0]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.0024119 , 0.        , 0.01592969, 0.        , 0.00105285,\n",
              "       0.01201967, 0.17945358, 0.00778265, 0.00782786, 0.60078794,\n",
              "       0.04109624, 0.776719  , 0.03663436], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErFut8Q-0LBS",
        "colab_type": "text"
      },
      "source": [
        "If the input values are of different ranges. They are not normalized or scaled. we get the result as nan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RncpZKmb1PCn",
        "colab_type": "text"
      },
      "source": [
        "Why Normalization is important.\n",
        "\n",
        "Consider two actual values of y as 300 and 0.1 and the predicted values be 290 and 0.4. \n",
        "\n",
        "The loss will be 100 amd 0.09 respectively.  0.3% and 300% respectively\n",
        "\n",
        "In this case during Gradient Descent, the importance will be given to higher value (100). So, the model will focus on larger or higher values and providing wrong results.\n",
        "\n",
        "Hence normalization is important."
      ]
    }
  ]
}